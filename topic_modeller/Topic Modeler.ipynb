{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bc0869",
   "metadata": {},
   "source": [
    "# Identificador de Arquivos Relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bb12d",
   "metadata": {},
   "source": [
    "Identifica os principais tópicos de uma lista de arquivos PDF e define se eles são relevantes ou não a um contexto pré-definido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd704d90",
   "metadata": {},
   "source": [
    "**Requisitos**: \n",
    "* Arquivo de configuração: config.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d6e89",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "1. [Importações e configurações](#p1)\n",
    "2. [Lista arquivos PDF a serem processados](#p2)\n",
    "3. [Função de pré-processamento](#p3)\n",
    "4. [Processa os arquivos PDF](#p4)\n",
    "5. [Modelador de tópico](#p5)\n",
    "6. [Busca de contexto](#p6)\n",
    "7. [Resultado](#p7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e0ad0",
   "metadata": {},
   "source": [
    "<a id=\"p1\"></a>\n",
    "## 1. Importações e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab91224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import PyPDF2\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "CONFIG_FILE = \"config.ini\"\n",
    "\n",
    "if not os.path.isfile(CONFIG_FILE):\n",
    "    print(f\"ERROR! Configuration file not found: {CONFIG_FILE}\")\n",
    "    exit(1)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_FILE)\n",
    "\n",
    "#Example = config[\"Settings\"][\"PdfFilesPath\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307dcba2",
   "metadata": {},
   "source": [
    "<a id=\"p2\"></a>\n",
    "## 2. Lista arquivos PDF a serem processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all PDFs from path\n",
    "pdf_search_path = config[\"Settings\"][\"PdfFilesPath\"]\n",
    "\n",
    "pdf_list = [os.path.join(pdf_search_path, f) for f in os.listdir(pdf_search_path) if os.path.isfile(os.path.join(pdf_search_path, f)) and f.endswith(\".pdf\")]\n",
    "\n",
    "# Read processed files\n",
    "processed_pdf_file = config[\"Settings\"][\"ProcessedPdfList\"]\n",
    "processed_pdf_list = list()\n",
    "with open(processed_pdf_file) as csvfile:\n",
    "    cvsreader = csv.reader(csvfile)\n",
    "    processed_pdf_list = [row[0] for row in cvsreader]\n",
    "\n",
    "# Smaller sample\n",
    "max_processed_files = int(config[\"Settings\"][\"MaxPdfFileProcessing\"])\n",
    "\n",
    "pdf_list = [pdf_file for pdf_file in pdf_list if pdf_file not in processed_pdf_list]\n",
    "pdf_list = pdf_list[:max_processed_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680adb8",
   "metadata": {},
   "source": [
    "<a id=\"p3\"></a>\n",
    "## 3. Função de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt')\n",
    "\n",
    "# Custom spacy tokenizer\n",
    "def process_tokens(tokens):\n",
    "    # Removable chars\n",
    "    extra_chars = list(\"%&!'()*#$˛ˇ\\\"?+-/˚ˇˆ˙˝˛˝˚˙˜˘\")\n",
    "   \n",
    "    # Remove stop words, puctuations, numbers, symbols, urls, emails, spaces, dates, time and extra chars\n",
    "    tokens = [token for token in tokens if \n",
    "                  not token.is_punct and \n",
    "                  not token.is_stop and \n",
    "                  not token.like_url and\n",
    "                  not token.like_email and\n",
    "                  not token.like_num and\n",
    "                  token.pos_ != \"NUM\" and \n",
    "                  token.pos_ != \"SYM\" and \n",
    "                  token.pos_ != \"SPACE\" and \n",
    "                  token.ent_type_ != \"DATE\" and\n",
    "                  token.ent_type_ != \"TIME\" and\n",
    "                  token.text not in extra_chars\n",
    "             ]\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    tokens = [token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_ for token in tokens ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43327b",
   "metadata": {},
   "source": [
    "<a id=\"p4\"></a>\n",
    "## 4. Processa os arquivos PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_content = []\n",
    "for pdf in pdf_list:\n",
    "    fopen = open(pdf,\"rb\")\n",
    "    \n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(fopen)\n",
    "        pdf_text = \"\"\n",
    "    \n",
    "        for p in range(pdf_reader.numPages):   \n",
    "            page = pdf_reader.getPage(p)\n",
    "            pdf_text += page.extractText()\n",
    "\n",
    "        tokens = nlp(pdf_text)\n",
    "        pdfs_content.append([pdf, process_tokens(tokens)])\n",
    "    except Exception as ex:\n",
    "        # TODO: Better process this errors\n",
    "        print(f\"PdfReadError! {pdf} - {type(ex).__name__}\")\n",
    "        pass\n",
    "\n",
    "    fopen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pdfs_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to data frame\n",
    "df = pd.DataFrame(pdfs_content, columns = ['PdfFile', 'Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0688b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82ff93",
   "metadata": {},
   "source": [
    "<a id=\"p5\"></a>\n",
    "## 5. Modelador de tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38854dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=spacy.lang.pt.stop_words.STOP_WORDS)\n",
    "\n",
    "dtm = cv.fit_transform(df['Content'])\n",
    "\n",
    "num_topics = int(config[\"Settings\"][\"NumberOfTopics\"])\n",
    "random_state = int(config[\"Settings\"][\"RandomState\"])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=num_topics,random_state=random_state)\n",
    "lda.fit(dtm)\n",
    "\n",
    "top_topics = list()\n",
    "for index,topic in enumerate(lda.components_):    \n",
    "    top_topics.append(set([cv.get_feature_names()[i] for i in topic.argsort()[-15:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check found topics\n",
    "i = 0\n",
    "for topic in top_topics:\n",
    "    print(i, topic)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b0f5d",
   "metadata": {},
   "source": [
    "<a id=\"p6\"></a>\n",
    "## 6. Busca de contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac54679",
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_topics = set(config[\"Settings\"][\"RelevantTopicKeyWords\"].split())\n",
    "\n",
    "found_topic_index = None\n",
    "\n",
    "i = 0\n",
    "for topic in top_topics:\n",
    "    if searched_topics.issubset(topic):\n",
    "        found_topic_index = i\n",
    "    i+=1\n",
    "\n",
    "\n",
    "topic_results = lda.transform(dtm)\n",
    "df['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(found_topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ac0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux function to set relevant topic\n",
    "def set_relevant (row, target_topic):\n",
    "    # target topic not found\n",
    "    if not target_topic:\n",
    "        return 0\n",
    "    \n",
    "    # target topic found\n",
    "    if row['Topic'] == target_topic:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218269bb",
   "metadata": {},
   "source": [
    "<a id=\"p7\"></a>\n",
    "## 7. Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4171d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataframenew column \"Relevant\" as 1 for matched topics and zero for not matched topics\n",
    "df['Relevant'] = df.apply(lambda row: set_relevant(row, found_topic_index), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final data frame columns\n",
    "formatted_df_output = df.to_csv(columns=[\"PdfFile\",\"Relevant\"], index=False, header=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(formatted_df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6087538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PdfFile','Relevant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60343b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to file\n",
    "with open(processed_pdf_file, \"a\") as output_file:\n",
    "    output_file.write(formatted_df_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
